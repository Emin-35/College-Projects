{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbsEGtijiJfD",
        "outputId": "3d9a1a6b-f23f-4d7b-9862-4b4d478fd928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "#Libraries that we used\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "#Download necessary corpora\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#From these libraries import necessary files\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.translate import Alignment, AlignedSent\n",
        "from nltk.translate.ibm1 import IBMModel1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create arrays for all the languages\n",
        "english_sentences = []\n",
        "turkish_sentences = []\n",
        "\n",
        "azerbaijani_sentences = []\n",
        "turkish2_sentences= []\n",
        "\n",
        "#Create a regex that changes words such as won't -> will not...\n",
        "replacement_patterns = [\n",
        "    (r'won\\'t', 'will not'),\n",
        "    (r'can\\'t', 'cannot'),\n",
        "    (r'i\\'m', 'i am'),\n",
        "    (r'ain\\'t', 'is not'),\n",
        "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
        "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
        "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
        "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
        "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
        "    (r'(\\w+)\\'d', r'\\g<1> would')\n",
        "]\n",
        "\n",
        "#This function taks a text and regex and changes words if any of them caught by the regex\n",
        "def apply_regex_patterns(text, replacement_patterns):\n",
        "  for pattern, replacement in replacement_patterns:\n",
        "    text = re.sub(pattern, replacement, text)\n",
        "  return text\n",
        "\n",
        "#Read english_turkish_corpus_final.txt and fill english_sentences, turkish_sentences arrays\n",
        "with open(\"english_turkish_corpus_final.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "  for line in file:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      english, turkish = line.split(\"*\")\n",
        "\n",
        "      #Modify English sentence\n",
        "      #Remove all the unnecessary signs\n",
        "      english = re.sub(r'[()\\[\\]{}.,;!?\\\\-]', '', english)\n",
        "      english = english.replace('[', '')\n",
        "      english = english.replace(']', '')\n",
        "      english = english.replace('.', '')\n",
        "      english = english.strip()\n",
        "      english = english.lower()\n",
        "      english = apply_regex_patterns(english, replacement_patterns)\n",
        "\n",
        "      #Modify Turkish sentence\n",
        "      turkish = re.sub(r'[()\\[\\]{}.,;!?\\\\-]', '', turkish)\n",
        "      turkish = turkish.replace('[', '')\n",
        "      turkish = turkish.replace(']', '')\n",
        "      turkish = turkish.replace('.', '')\n",
        "      turkish = turkish.strip()\n",
        "      turkish = turkish.lower()\n",
        "      turkish = apply_regex_patterns(turkish, replacement_patterns)\n",
        "\n",
        "      #Append the new sentences into corresponding arrays\n",
        "      english_sentences.append(english.strip())\n",
        "      turkish_sentences.append(turkish.strip())\n",
        "\n",
        "\n",
        "with open(\"turkish_azerbaijani_corpus_final.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "  for line in file:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      turkish2, azerbaijani = line.split(\"*\")\n",
        "\n",
        "      #Modify Turkish sentence\n",
        "      turkish2 = re.sub(r'[()\\[\\]{}.,;!?\\\\-]', '', turkish2)\n",
        "      turkish2 = turkish2.replace('[', '')\n",
        "      turkish2 = turkish2.replace(']', '')\n",
        "      turkish2 = turkish2.replace('.', '')\n",
        "      turkish2 = turkish2.strip()\n",
        "      turkish2 = turkish2.lower()\n",
        "      turkish2 = apply_regex_patterns(turkish2, replacement_patterns)\n",
        "\n",
        "      #Modify Azeri sentence\n",
        "      azerbaijani = re.sub(r'[()\\[\\]{}.,;!?\\\\-]', '', azerbaijani)\n",
        "      azerbaijani = azerbaijani.replace('[', '')\n",
        "      azerbaijani = azerbaijani.replace(']', '')\n",
        "      azerbaijani = azerbaijani.replace('.', '')\n",
        "      azerbaijani = azerbaijani.strip()\n",
        "      azerbaijani = azerbaijani.lower()\n",
        "      azerbaijani = apply_regex_patterns(azerbaijani, replacement_patterns)\n",
        "\n",
        "      #Append the new sentences into corresponding arrays\n",
        "      azerbaijani_sentences.append(azerbaijani.strip())\n",
        "      turkish2_sentences.append(turkish2.strip())"
      ],
      "metadata": {
        "id": "j7eXVkcOiZUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_match(input_sentence, src_sentences, target_sentences):\n",
        "  #Use TF-IDF vectorizer to convert sentences to vectors\n",
        "  vectorizer = TfidfVectorizer(stop_words='english')\n",
        "  sentence_vectors = vectorizer.fit_transform(src_sentences + [input_sentence])\n",
        "\n",
        "  #Find the similarity between the input sentence and the corpus\n",
        "  input_vector = sentence_vectors[-1]\n",
        "  score_of_similarity = np.dot(sentence_vectors[:-1], input_vector.T).toarray().flatten()\n",
        "\n",
        "  #Sort the similarity scores and get the top 10 sentences\n",
        "  top_place = np.argsort(score_of_similarity)[-10:]\n",
        "\n",
        "  #Retrieve the top 10 similar sentences and their Target translations\n",
        "  similar_sentences = [src_sentences[i] for i in top_place]\n",
        "  target_sentences = [target_sentences[i] for i in top_place]\n",
        "\n",
        "  #Turn them into tokenize versions\n",
        "  target_tokens = [nltk.word_tokenize(s) for s in target_sentences]\n",
        "  similar_tokens = [nltk.word_tokenize(s) for s in similar_sentences]\n",
        "  #Add them to list\n",
        "  optimal_sentences = list(zip(similar_tokens, target_tokens))\n",
        "  \n",
        "  print(\"similar sentences in the corpus:\\n\", optimal_sentences)\n",
        "  return optimal_sentences  #Return the top 10 similar source sentences"
      ],
      "metadata": {
        "id": "kWtJQr0xih0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function takes an array of sentences and adds them into Alignment Table so that every word has an alignment\n",
        "def convert_to_aligned_sents(parallel_sentences):\n",
        "  aligned_sents = []\n",
        "  for initial_tokens, target_tokens in parallel_sentences:\n",
        "    #Convert the initial tokens and target tokens into sentences by joining the tokens with spaces:\n",
        "    initial_sentence = ' '.join(initial_tokens)\n",
        "    target_sentence = ' '.join(target_tokens)\n",
        "\n",
        "    #Create an alignment object called alignment by mapping each token position to itself using a list comprehension\n",
        "    alignment = Alignment([(i, i) for i in range(min(len(initial_tokens), len(target_tokens)))])\n",
        "    aligned_sent = AlignedSent(initial_tokens, target_tokens, alignment)\n",
        "    #Append the aligned sentence to the aligned_sents list\n",
        "    aligned_sents.append(aligned_sent)\n",
        "  return aligned_sents"
      ],
      "metadata": {
        "id": "P1E5lr7njOQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence):\n",
        "  # Tokenize the input sentence\n",
        "  sentence_tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "  # Create a set to keep track of checked words\n",
        "  translation_tokens = []\n",
        "  checked_words = set()\n",
        "\n",
        "  #Iterate over each token in the sentence tokens\n",
        "  for token in sentence_tokens:\n",
        "    #Check if the token has already been checked\n",
        "    if token in checked_words:\n",
        "      #If it has been checked, append an empty string to the translation tokens list and continue to the next token\n",
        "      translation_tokens.append(\"\")\n",
        "      continue\n",
        "    \n",
        "    #Initialize variables for the best translation and maximum probability\n",
        "    best_translation = \"\"\n",
        "    max_prob = 0.0\n",
        "\n",
        "    # Iterate over each translation token for the current token\n",
        "    for idx, tr_token in enumerate(ibm1.translation_table[token]):\n",
        "\n",
        "      #Check if the probability of the translation token is greater than the current maximum probability\n",
        "      if ibm1.translation_table[token][tr_token] > max_prob:\n",
        "        #If it is greater, update the maximum probability and set the best translation to the current translation token\n",
        "        max_prob = ibm1.translation_table[token][tr_token]\n",
        "        best_translation = tr_token\n",
        "\n",
        "      #If the probability of the translation token is equal to the current maximum probability\n",
        "      elif ibm1.translation_table[token][tr_token] == max_prob:\n",
        "        #Check if the best translation is empty\n",
        "        if not best_translation:\n",
        "          #If it is empty, set the best translation to the current translation token\n",
        "          best_translation = tr_token\n",
        "        else:\n",
        "          #If it is not empty, append the current translation token to the best translation with a space separator\n",
        "          best_translation = best_translation + \" \" + tr_token\n",
        "    \n",
        "    # Check if a best translation was found\n",
        "    if best_translation:\n",
        "        #If a best translation was found, append it to the translation tokens list and add it to the checked words set\n",
        "        translation_tokens.append(best_translation)\n",
        "        checked_words.add(best_translation)\n",
        "\n",
        "  #Return the final translation tokens\n",
        "  return translation_tokens"
      ],
      "metadata": {
        "id": "OWelYvkGkvme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove substrings\n",
        "def remove_repeating_variables(array):\n",
        "  unique_variables = []\n",
        "  #If the given array has not unique variables, remove them\n",
        "  for item in array:\n",
        "    if item not in unique_variables:\n",
        "      unique_variables.append(item)\n",
        "  return ' '.join(unique_variables)"
      ],
      "metadata": {
        "id": "1SgZuEPHk0Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ask the user a question and save the answer\n",
        "user_option = input(\"Please select 1 or 2: \\n 1 For English to Turksih \\n 2 For Azerbaijani to Turkish \\n\")\n",
        "\n",
        "#If the answer is not 1 or 2 ask him again\n",
        "while user_option != '1' and user_option != '2':\n",
        "  user_option = input(\"Please select 1 or 2: \\n 1 For English to Turksih \\n 2 For Azerbaijani to Turkish \\n\")\n",
        "\n",
        "#If the answer is 1 call the necessasary functions for English to Turkish translations\n",
        "if( int(user_option) == 1):\n",
        "  user_sentence = input(\"Please enter an English sentence for translation to Turkish \\n\")\n",
        "  print(\"Your sentence: \", user_sentence)\n",
        "  parallel_sentences = find_optimal_match(user_sentence, english_sentences, turkish_sentences)\n",
        "  aligned_sents = convert_to_aligned_sents(parallel_sentences)\n",
        "  ibm1 = IBMModel1(aligned_sents, iterations=10)\n",
        "\n",
        "  translation = translate_sentence(user_sentence)\n",
        "  fixed_translation = remove_repeating_variables(translation)\n",
        "  print(\"Input:\", user_sentence)\n",
        "  print(\"Translation:\", fixed_translation)\n",
        "\n",
        "#Else if the answer is 2 call the necessasary functions for Azerbaijani to Turkish translations\n",
        "elif( int(user_option) == 2):\n",
        "  user_sentence = input(\"Please enter an Azerbaijani sentence for translation to Turkish \\n\")\n",
        "  parallel_sentences = find_optimal_match(user_sentence, azerbaijani_sentences, turkish2_sentences)\n",
        "  aligned_sents = convert_to_aligned_sents(parallel_sentences)\n",
        "  ibm1 = IBMModel1(aligned_sents, iterations=10)\n",
        "\n",
        "  translation = translate_sentence(user_sentence)\n",
        "  fixed_translation = remove_repeating_variables(translation)\n",
        "  print(\"Input:\", user_sentence)\n",
        "  print(\"Translation:\", fixed_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp2VXTQ8lCt-",
        "outputId": "1aac7371-467c-4e84-f81a-394735edb2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please select 1 or 2: \n",
            " 1 For English to Turksih \n",
            " 2 For Azerbaijani to Turkish \n",
            "2\n",
            "Please enter an Azerbaijani sentence for translation to Turkish \n",
            "Bu kitablar mənim sözümü tutmamaqda ittiham etdilər\n",
            "similar sentences in the corpus:\n",
            " [(['bunu', 'dörd', 'gündə', 'etdilər'], ['bunu', 'dört', 'günde', 'yaptılar']), (['1992ci', 'ildə', 'kitablar', 'nəşr', 'etməyə', 'başladılar'], ['1992', 'woulde', 'kitap', 'yayınlamaya', 'başladılar']), (['kitabxanamızda', 'çoxlu', 'ingiliscə', 'kitablar', 'var'], ['kütüphanemizde', 'çok', 'sayıda', 'i̇ngilizce', 'kitap', 'var']), (['sözümü', 'kəsmə', 'danışdığımı', 'görmürsən'], ['sözümü', 'kesme', 'konuştuğumu', 'görmüyor', 'musun']), (['adam', 'məni', 'məsuliyyətsizlikdə', 'ittiham', 'etdi'], ['adam', 'beni', 'sorumsuz', 'olmakla', 'suçladı']), (['nə', 'olursa', 'olsun', 'sözümü', 'tutacam'], ['ne', 'olursa', 'olsun', 'sözümü', 'tutacağım']), (['o', 'onu', 'ehtiyatsızlıqda', 'ittiham', 'edib'], ['onu', 'dikkatsiz', 'olmakla', 'suçladı']), (['belə', 'kitablar', 'onun', 'üçün', 'çox', 'çətindir'], ['bu', 'tür', 'kitaplar', 'onun', 'için', 'çok', 'zor']), (['bu', 'kitablar', 'mənim', 'bu', 'kitablar', 'da', 'onundur'], ['bu', 'kitaplar', 'benim', 've', 'bu', 'kitaplar', 'onun']), (['məni', 'sözümü', 'tutmamaqda', 'ittiham', 'etdilər'], ['beni', 'sözümü', 'tutmamakla', 'suçladılar'])]\n",
            "Input: Bu kitablar mənim sözümü tutmamaqda ittiham etdilər\n",
            "Translation: kitap benim ve sözümü tutmamakla suçladılar olmakla suçladı\n"
          ]
        }
      ]
    }
  ]
}